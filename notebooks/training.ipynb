{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granite-8B Fine-Tuning: Iterative Teacher-Student Distillation (Colab Native)\n",
    "\n",
    "**Target**: Embedded Automotive Code Generation (AVB/TSN)  \n",
    "**Model**: IBM Granite-8B-Code-Instruct-128K  \n",
    "**Teacher**: Claude Sonnet 4.5 via Amazon Bedrock  \n",
    "**Compute**: Google Colab A100 GPU (High RAM)  \n",
    "**Data**: AWS S3  \n",
    "**Storage**: AWS S3 (checkpoints & model output)\n",
    "\n",
    "## Pipeline Stages\n",
    "1. Environment Setup & GPU Optimization\n",
    "2. Credentials (HuggingFace + AWS) & Configuration\n",
    "3. Data Preparation (S3 → Training JSONL)\n",
    "4. Teacher Output Generation (Bedrock Claude)\n",
    "5. QLoRA Fine-Tuning with Iterative Distillation\n",
    "6. Evaluation & Quality Metrics\n",
    "7. Model Export to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (S3 + Bedrock via boto3, no SageMaker)\n",
    "!pip install -q torch transformers datasets accelerate peft trl bitsandbytes \\\n",
    "    sentencepiece protobuf boto3 python-dotenv pyyaml tqdm \\\n",
    "    rouge-score sacrebleu evaluate scipy huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No .env file found — will prompt for credentials interactively\n",
      "Repo already cloned at /content/fine_tuning_IBM_8B_v2\n",
      "Project root: /content/fine_tuning_IBM_8B_v2\n",
      "Output S3 bucket: s3://granite-8b-training-outputs/runs/\n",
      "Contents: ['README.md', 'docker', 'PLAN.md', 'training', 'evaluation', 'pytest.ini', 'scripts', '.gitignore', 'context_db', '.git', 'checkpoints', 'data', 'tests', 'config.yaml', 'aws', 'requirements.txt', 'models', 'PIPELINE_STATE_MACHINE.md', 'output', 'run_gpu_tests.py']\n"
     ]
    }
   ],
   "source": [
    "# Project setup, credentials loading, and S3 output configuration\n",
    "import os, sys\n",
    "\n",
    "# Load .env file if available (VS Code / local development)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    # Try repo root .env first, then parent dirs\n",
    "    for env_path in ['/content/fine_tuning_IBM_8B_v2/.env', '.env', '../.env']:\n",
    "        if os.path.exists(env_path):\n",
    "            load_dotenv(env_path, override=True)\n",
    "            print(f\"Loaded credentials from {env_path}\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"No .env file found — will prompt for credentials interactively\")\n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed — will prompt for credentials interactively\")\n",
    "\n",
    "# Helper: get secret from env var, .env, or interactive prompt\n",
    "def get_secret(key, default=None):\n",
    "    \"\"\"Get credential from env var (incl. .env) or prompt interactively.\"\"\"\n",
    "    val = os.environ.get(key)\n",
    "    if val:\n",
    "        return val\n",
    "    if default is not None:\n",
    "        return default\n",
    "    # Interactive fallback — works in VS Code + Colab extension\n",
    "    import getpass\n",
    "    val = getpass.getpass(f\"Enter {key}: \")\n",
    "    if val:\n",
    "        os.environ[key] = val  # Cache for rest of session\n",
    "    return val or default\n",
    "\n",
    "# S3 bucket for training outputs (models, checkpoints, summaries)\n",
    "OUTPUT_BUCKET = 'granite-8b-training-outputs'\n",
    "S3_OUTPUT_PREFIX = 'runs'\n",
    "\n",
    "# Clone private repo using GitHub token\n",
    "GITHUB_TOKEN = get_secret('GITHUB_TOKEN')\n",
    "if not GITHUB_TOKEN:\n",
    "    raise RuntimeError(\"GITHUB_TOKEN not set. Add it to .env, env var, or enter when prompted.\")\n",
    "REPO_URL = f'https://{GITHUB_TOKEN}@github.com/sriramach2000/fine_tuning_IBM_8B_v2.git'\n",
    "PROJECT_ROOT = '/content/fine_tuning_IBM_8B_v2'\n",
    "\n",
    "if not os.path.exists(PROJECT_ROOT):\n",
    "    !git clone {REPO_URL} {PROJECT_ROOT}\n",
    "else:\n",
    "    print(f\"Repo already cloned at {PROJECT_ROOT}\")\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Re-load .env from cloned repo if it exists\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    repo_env = os.path.join(PROJECT_ROOT, '.env')\n",
    "    if os.path.exists(repo_env):\n",
    "        load_dotenv(repo_env, override=True)\n",
    "        print(f\"Loaded credentials from {repo_env}\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Create local data/output directories\n",
    "for d in ['data/raw', 'data/processed', 'data/splits', 'data/teacher_outputs',\n",
    "          'data/eval', 'output/notebook_run', 'models/notebook_output', 'checkpoints']:\n",
    "    os.makedirs(os.path.join(PROJECT_ROOT, d), exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Output S3 bucket: s3://{OUTPUT_BUCKET}/{S3_OUTPUT_PREFIX}/\")\n",
    "print(f\"Contents: {os.listdir(PROJECT_ROOT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── All Imports ──────────────────────────────────────────────────────\n",
    "# Standard library\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Core ML / DL\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "# HuggingFace ecosystem\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi, login\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# AWS\n",
    "import boto3\n",
    "\n",
    "# Project-local imports (requires sys.path from cell above)\n",
    "from evaluation.code_quality_metrics import CodeQualityEvaluator\n",
    "from training.iterative_distillation import (\n",
    "    IterativeDistillationTrainer,\n",
    "    DistillationConfig,\n",
    ")\n",
    "from training.train_granite_qlora import (\n",
    "    NaNInfDetectionCallback,\n",
    "    CustomEarlyStoppingCallback,\n",
    "    format_chat_template,\n",
    ")\n",
    "from scripts.generate_teacher_outputs import (\n",
    "    BedrockTeacherGenerator,\n",
    "    create_automotive_system_prompt,\n",
    "    create_sample_prompts,\n",
    ")\n",
    "from scripts.run_iterative_pipeline import create_sample_eval_prompts\n",
    "from scripts.prepare_automotive_data import AutomotiveDataPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREDENTIAL VALIDATION\n",
      "============================================================\n",
      "[OK] AWS STS — Account 122634724608\n",
      "[OK] S3 data bucket (granite-8b-unified-automotive-data)\n",
      "[OK] S3 output bucket (granite-8b-training-outputs)\n",
      "[OK] Bedrock (Claude Sonnet 4.5)\n",
      "[OK] HuggingFace — user \"sriramach\"\n",
      "[OK] GitHub — repo accessible\n",
      "============================================================\n",
      "All credentials valid. Safe to proceed.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ── Validate All API Keys & Credentials ────────────────────────────────\n",
    "print(\"=\" * 60)\n",
    "print(\"CREDENTIAL VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure all credentials are loaded (prompts if not in env/.env)\n",
    "for key in ['AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY', 'AWS_REGION',\n",
    "            'AMAZON_BEDROCK_MODEL_API_KEY', 'HF_TOKEN', 'GITHUB_TOKEN']:\n",
    "    val = get_secret(key, default=('us-east-1' if key == 'AWS_REGION' else None))\n",
    "    if val:\n",
    "        os.environ[key] = val\n",
    "\n",
    "all_ok = True\n",
    "\n",
    "# 1. AWS Identity\n",
    "try:\n",
    "    sts = boto3.client('sts', region_name=os.environ.get('AWS_REGION', 'us-east-1'))\n",
    "    identity = sts.get_caller_identity()\n",
    "    print(f\"[OK] AWS STS — Account {identity['Account']}\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] AWS STS — {e}\")\n",
    "    all_ok = False\n",
    "\n",
    "# 2. S3 data bucket\n",
    "try:\n",
    "    s3_test = boto3.client('s3', region_name=os.environ.get('AWS_REGION', 'us-east-1'))\n",
    "    s3_test.head_bucket(Bucket='granite-8b-unified-automotive-data')\n",
    "    print(\"[OK] S3 data bucket (granite-8b-unified-automotive-data)\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] S3 data bucket — {e}\")\n",
    "    all_ok = False\n",
    "\n",
    "# 3. S3 output bucket\n",
    "try:\n",
    "    s3_test.head_bucket(Bucket=OUTPUT_BUCKET)\n",
    "    print(f\"[OK] S3 output bucket ({OUTPUT_BUCKET})\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] S3 output bucket ({OUTPUT_BUCKET}) — {e}\")\n",
    "    print(\"       Create it: aws s3 mb s3://\" + OUTPUT_BUCKET)\n",
    "    all_ok = False\n",
    "\n",
    "# 4. Bedrock\n",
    "try:\n",
    "    bedrock_test = boto3.client('bedrock-runtime', region_name=os.environ.get('AWS_REGION', 'us-east-1'))\n",
    "    test_body = json.dumps({\n",
    "        'anthropic_version': 'bedrock-2023-05-31',\n",
    "        'max_tokens': 16,\n",
    "        'messages': [{'role': 'user', 'content': 'Say OK'}]\n",
    "    })\n",
    "    resp = bedrock_test.invoke_model(\n",
    "        modelId='us.anthropic.claude-sonnet-4-5-20250929-v1:0',\n",
    "        body=test_body, contentType='application/json', accept='application/json',\n",
    "    )\n",
    "    print(\"[OK] Bedrock (Claude Sonnet 4.5)\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Bedrock — {e}\")\n",
    "    all_ok = False\n",
    "\n",
    "# 5. HuggingFace\n",
    "try:\n",
    "    hf_token = os.environ.get('HF_TOKEN')\n",
    "    api = HfApi(token=hf_token)\n",
    "    hf_info = api.whoami()\n",
    "    print(f\"[OK] HuggingFace — user \\\"{hf_info['name']}\\\"\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] HuggingFace — {e}\")\n",
    "    all_ok = False\n",
    "\n",
    "# 6. GitHub (repo access)\n",
    "try:\n",
    "    import urllib.request\n",
    "    gh_token = os.environ.get('GITHUB_TOKEN')\n",
    "    req = urllib.request.Request(\n",
    "        'https://api.github.com/repos/sriramach2000/fine_tuning_IBM_8B_v2',\n",
    "        headers={'Authorization': f'token {gh_token}', 'User-Agent': 'colab'}\n",
    "    )\n",
    "    urllib.request.urlopen(req)\n",
    "    print(\"[OK] GitHub — repo accessible\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] GitHub — {e}\")\n",
    "    all_ok = False\n",
    "\n",
    "print(\"=\" * 60)\n",
    "if all_ok:\n",
    "    print(\"All credentials valid. Safe to proceed.\")\n",
    "else:\n",
    "    print(\"WARNING: Some credentials failed. Fix before continuing.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "VRAM: 85.2 GB\n",
      "\n",
      "--- GPU Optimization ---\n",
      "TF32 enabled for matmul and cuDNN\n",
      "cuDNN benchmark mode enabled\n",
      "CUDA memory: expandable_segments enabled\n",
      "\n",
      "Available VRAM: 84.7 / 85.2 GB\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU availability - requires A100 + High RAM runtime\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    vram_gb = getattr(props, 'total_memory', getattr(props, 'total_mem', 0)) / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {vram_gb:.1f} GB\")\n",
    "\n",
    "    if 'A100' not in gpu_name:\n",
    "        print(f\"\\n⚠ WARNING: Expected A100 but got {gpu_name}.\")\n",
    "        print(\"Go to Runtime > Change runtime type > A100 GPU, High RAM\")\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"No GPU detected! Go to Runtime > Change runtime type > \"\n",
    "        \"Hardware accelerator: A100 GPU, Runtime shape: High RAM\"\n",
    "    )\n",
    "\n",
    "# --- A100 GPU Optimization ---\n",
    "print(\"\\n--- GPU Optimization ---\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "print(\"TF32 enabled for matmul and cuDNN\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(\"cuDNN benchmark mode enabled\")\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "print(\"CUDA memory: expandable_segments enabled\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "print(f\"\\nAvailable VRAM: {free_mem / 1e9:.1f} / {total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Credentials (HuggingFace + AWS) & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF: Logged in\n",
      "\n",
      "HF_TOKEN: set\n",
      "AWS_ACCESS_KEY_ID: set\n",
      "AWS_SECRET_ACCESS_KEY: set\n",
      "AWS_REGION: us-east-1\n",
      "BEDROCK_API_KEY: set\n",
      "\n",
      "S3 bucket accessible!\n"
     ]
    }
   ],
   "source": [
    "# Authenticate with HuggingFace + AWS\n",
    "# --- HuggingFace ---\n",
    "try:\n",
    "    HF_TOKEN = get_secret('HF_TOKEN')\n",
    "    login(token=HF_TOKEN)\n",
    "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "    print(\"HF: Logged in\")\n",
    "except Exception:\n",
    "    HF_TOKEN = input(\"Enter your HuggingFace token: \")\n",
    "    login(token=HF_TOKEN)\n",
    "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "# --- AWS (S3 + Bedrock) ---\n",
    "for key in ['AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY', 'AWS_REGION', 'AMAZON_BEDROCK_MODEL_API_KEY']:\n",
    "    try:\n",
    "        val = get_secret(key)\n",
    "        if val:\n",
    "            os.environ[key] = val\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Fallback: set AWS_REGION default\n",
    "if not os.environ.get('AWS_REGION'):\n",
    "    os.environ['AWS_REGION'] = 'us-east-1'\n",
    "\n",
    "# Verify\n",
    "print(f\"\\nHF_TOKEN: {'set' if os.environ.get('HF_TOKEN') else 'NOT SET'}\")\n",
    "print(f\"AWS_ACCESS_KEY_ID: {'set' if os.environ.get('AWS_ACCESS_KEY_ID') else 'NOT SET'}\")\n",
    "print(f\"AWS_SECRET_ACCESS_KEY: {'set' if os.environ.get('AWS_SECRET_ACCESS_KEY') else 'NOT SET'}\")\n",
    "print(f\"AWS_REGION: {os.environ.get('AWS_REGION')}\")\n",
    "print(f\"BEDROCK_API_KEY: {'set' if os.environ.get('AMAZON_BEDROCK_MODEL_API_KEY') else 'NOT SET'}\")\n",
    "\n",
    "# Quick S3 connectivity check\n",
    "try:\n",
    "    s3 = boto3.client('s3', region_name=os.environ['AWS_REGION'])\n",
    "    s3.head_bucket(Bucket='granite-8b-unified-automotive-data')\n",
    "    print(\"\\nS3 bucket accessible!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nS3 access error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: granite-8b-avb-tsn-finetuning\n",
      "Model: ibm-granite/granite-8b-code-instruct-128k\n",
      "Checkpoints: /content/fine_tuning_IBM_8B_v2/checkpoints\n",
      "Output bucket: s3://granite-8b-training-outputs/runs/\n"
     ]
    }
   ],
   "source": [
    "# Load project config and override cloud-specific settings for Colab\n",
    "config_path = os.path.join(PROJECT_ROOT, 'config.yaml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Override paths for Colab environment\n",
    "config['paths'] = {\n",
    "    'data': {\n",
    "        'processed_dir': os.path.join(PROJECT_ROOT, 'data', 'processed'),\n",
    "        'splits_dir': os.path.join(PROJECT_ROOT, 'data', 'splits'),\n",
    "        'teacher_outputs_dir': os.path.join(PROJECT_ROOT, 'data', 'teacher_outputs'),\n",
    "        'distillation_dir': os.path.join(PROJECT_ROOT, 'data', 'distillation'),\n",
    "    },\n",
    "    'training': {\n",
    "        'output_dir': os.path.join(PROJECT_ROOT, 'output', 'notebook_run'),\n",
    "        'checkpoint_dir': os.path.join(PROJECT_ROOT, 'checkpoints'),\n",
    "        'logs_dir': os.path.join(PROJECT_ROOT, 'logs'),\n",
    "    },\n",
    "}\n",
    "\n",
    "splits_dir = config['paths']['data']['splits_dir']\n",
    "\n",
    "print(f\"Project: {config['project']['name']}\")\n",
    "print(f\"Model: {config['model']['name']}\")\n",
    "print(f\"Checkpoints: {config['paths']['training']['checkpoint_dir']}\")\n",
    "print(f\"Output bucket: s3://{OUTPUT_BUCKET}/{S3_OUTPUT_PREFIX}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation (S3 → Training JSONL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download from S3 and process into training JSONL\n# All processing runs locally on Colab (no SageMaker)\nimport time\n\npipeline = AutomotiveDataPipeline(\n    s3_bucket=config['aws']['s3']['bucket_name'],\n    region=os.environ.get('AWS_REGION', 'us-east-1'),\n    local_data_dir=os.path.join(PROJECT_ROOT, 'data', 'raw'),\n    processed_dir=os.path.join(PROJECT_ROOT, 'data', 'processed'),\n    splits_dir=splits_dir,\n)\n\n# Download from S3 and process on Colab (full dataset)\nstart_time = time.time()\ntrain_examples, val_examples = pipeline.run_pipeline(\n    download_data=True,\n    max_files_per_type=0,  # 0 = no limit (process all files)\n    train_ratio=0.9,\n)\nelapsed = time.time() - start_time\n\nprint(f\"\\nTrain examples: {len(train_examples)}\")\nprint(f\"Val examples: {len(val_examples)}\")\nprint(f\"Processing time: {elapsed/60:.1f} minutes\")\n\n# Upload processed splits to S3 for persistence across runtime resets\ns3_splits = boto3.client('s3', region_name=os.environ.get('AWS_REGION', 'us-east-1'))\nsplits_prefix = f\"{S3_OUTPUT_PREFIX}/data/splits\"\n\nfor split_name in ['train.jsonl', 'val.jsonl']:\n    local_path = os.path.join(splits_dir, split_name)\n    if os.path.exists(local_path):\n        s3_key = f\"{splits_prefix}/{split_name}\"\n        size_mb = os.path.getsize(local_path) / 1e6\n        print(f\"Uploading {split_name} ({size_mb:.1f} MB) -> s3://{OUTPUT_BUCKET}/{s3_key}\")\n        s3_splits.upload_file(local_path, OUTPUT_BUCKET, s3_key)\n\nprint(f\"Splits uploaded to s3://{OUTPUT_BUCKET}/{splits_prefix}/\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training example:\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Implement the function 'if' for automotive embedded systems. Context: IEEE 802.1Qbv Time-Aware Shaper\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"else if (si.instance_state == DDS::NOT_ALIVE_NO_WRITERS_INSTANCE_STATE) {\\n        ACE_DEBUG((LM_DEBUG, ACE_TEXT(\\\"%N:%l: INFO: instance is unregistered\\\\n\\\")));\\n\\n      }\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Inspect a sample training example\n",
    "train_file = Path(splits_dir) / 'train.jsonl'\n",
    "\n",
    "if train_file.exists():\n",
    "    with open(train_file, 'r') as f:\n",
    "        first_example = json.loads(f.readline())\n",
    "    print(\"Sample training example:\")\n",
    "    print(json.dumps(first_example, indent=2)[:500])\n",
    "else:\n",
    "    print(f\"No training file at {train_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Teacher Output Generation (Bedrock Claude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock teacher model (Claude Sonnet 4.5)\n",
    "teacher = BedrockTeacherGenerator(\n",
    "    model_id=config['distillation']['teacher_model'],\n",
    "    region=os.environ.get('AWS_REGION', 'us-east-1'),\n",
    "    max_tokens=config['distillation']['max_teacher_tokens'],\n",
    "    temperature=config['aws']['bedrock']['temperature'],\n",
    ")\n",
    "\n",
    "# Connectivity test\n",
    "test_result = teacher.generate_response(\n",
    "    prompt=\"Generate a C struct for a TSN gate control list entry with priority, gate state, and time interval fields.\",\n",
    "    system_prompt=create_automotive_system_prompt(),\n",
    ")\n",
    "\n",
    "if test_result['success']:\n",
    "    print(\"Bedrock teacher model connected!\")\n",
    "    print(f\"\\nResponse preview:\\n{test_result['response'][:300]}...\")\n",
    "else:\n",
    "    print(f\"Error: {test_result['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate teacher outputs for sample prompts\n",
    "sample_prompts = create_sample_prompts()\n",
    "system_prompt = create_automotive_system_prompt()\n",
    "\n",
    "output_file = os.path.join(PROJECT_ROOT, 'data', 'teacher_outputs', 'bedrock_outputs.jsonl')\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "results = teacher.generate_batch(\n",
    "    prompts=sample_prompts,\n",
    "    system_prompt=system_prompt,\n",
    "    output_file=output_file,\n",
    "    checkpoint_interval=5,\n",
    "    max_workers=5,\n",
    ")\n",
    "\n",
    "successful = sum(1 for r in results if r['success'])\n",
    "print(f\"\\nGenerated {successful}/{len(results)} teacher outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. QLoRA Fine-Tuning with Iterative Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and validation datasets\n",
    "train_jsonl = os.path.join(splits_dir, 'train.jsonl')\n",
    "val_jsonl = os.path.join(splits_dir, 'val.jsonl')\n",
    "\n",
    "train_dataset = load_dataset('json', data_files=train_jsonl, split='train')\n",
    "val_dataset = load_dataset('json', data_files=val_jsonl, split='train')\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Val dataset: {len(val_dataset)} examples\")\n",
    "print(f\"Columns: {train_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Granite-8B with QLoRA\n",
    "MODEL_NAME = config['model']['name']\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=config['qlora']['bnb_4bit_quant_type'],\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=config['qlora']['bnb_4bit_use_double_quant'],\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation='eager',\n",
    "    token=os.environ.get('HF_TOKEN'),\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=config['qlora']['lora_r'],\n",
    "    lora_alpha=config['qlora']['lora_alpha'],\n",
    "    lora_dropout=config['qlora']['lora_dropout'],\n",
    "    target_modules=config['qlora']['lora_target_modules'],\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=os.environ.get('HF_TOKEN'))\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTrainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"VRAM used: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up SFTTrainer\n",
    "output_dir = os.path.join(PROJECT_ROOT, 'output', 'notebook_run')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=config['training']['num_epochs'],\n",
    "    per_device_train_batch_size=config['training']['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=config['training']['per_device_train_batch_size'],\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "    learning_rate=config['training']['learning_rate'],\n",
    "    lr_scheduler_type=config['training']['lr_scheduler_type'],\n",
    "    warmup_ratio=config['training']['warmup_ratio'],\n",
    "    weight_decay=config['training']['weight_decay'],\n",
    "    max_grad_norm=config['training']['max_grad_norm'],\n",
    "    optim=config['training']['optim'],\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_dir=os.path.join(output_dir, 'logs'),\n",
    "    logging_steps=config['training']['logging_steps'],\n",
    "    logging_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=config['training']['eval_steps'],\n",
    "    save_strategy='steps',\n",
    "    save_steps=config['training']['save_steps'],\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "def formatting_func(example):\n",
    "    return format_chat_template(example, tokenizer)\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=config['model']['max_seq_length'],\n",
    "    callbacks=[\n",
    "        NaNInfDetectionCallback(),\n",
    "        CustomEarlyStoppingCallback(\n",
    "            patience=config['training']['early_stopping_patience'],\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"SFTTrainer ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize iterative distillation components\n",
    "evaluator = CodeQualityEvaluator(\n",
    "    strict_mode=True,\n",
    "    quality_threshold=config['distillation']['min_score_threshold'],\n",
    ")\n",
    "\n",
    "distillation_config = DistillationConfig(\n",
    "    quality_threshold=config['distillation']['min_score_threshold'],\n",
    "    convergence_threshold=config['distillation']['convergence_threshold'],\n",
    "    convergence_patience=3,\n",
    "    teacher_model=config['distillation']['teacher_model'],\n",
    "    max_corrections_per_epoch=500,\n",
    "    max_parallel_teacher_calls=5,\n",
    "    eval_samples_per_epoch=200,\n",
    "    output_dir=output_dir,\n",
    "    model_dir=os.path.join(PROJECT_ROOT, 'models', 'notebook_output'),\n",
    "    corrections_dir=os.path.join(output_dir, 'corrections'),\n",
    ")\n",
    "\n",
    "distillation_trainer = IterativeDistillationTrainer(\n",
    "    student_model=model,\n",
    "    student_tokenizer=tokenizer,\n",
    "    teacher_generator=teacher,  # BedrockTeacherGenerator\n",
    "    quality_evaluator=evaluator,\n",
    "    config=distillation_config,\n",
    "    trainer=sft_trainer,\n",
    ")\n",
    "\n",
    "print(\"Iterative distillation trainer ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load eval prompts\n",
    "eval_prompts_file = Path(PROJECT_ROOT) / 'data' / 'eval' / 'eval_prompts.jsonl'\n",
    "if eval_prompts_file.exists():\n",
    "    with open(eval_prompts_file, 'r') as f:\n",
    "        eval_prompts = [json.loads(line) for line in f]\n",
    "    print(f\"Loaded {len(eval_prompts)} eval prompts from file\")\n",
    "else:\n",
    "    eval_prompts = create_sample_eval_prompts()\n",
    "    print(f\"Using {len(eval_prompts)} sample eval prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run iterative distillation training loop\n",
    "MAX_EPOCHS = config['distillation']['max_iterations']\n",
    "EVAL_SAMPLES = min(len(eval_prompts), 200)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ITERATIVE TEACHER-STUDENT DISTILLATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Max epochs: {MAX_EPOCHS}\")\n",
    "print(f\"Eval samples/epoch: {EVAL_SAMPLES}\")\n",
    "print(f\"Quality threshold: {distillation_config.quality_threshold}\")\n",
    "print(f\"Convergence target: {distillation_config.convergence_threshold}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    epoch_eval = eval_prompts[:EVAL_SAMPLES]\n",
    "\n",
    "    metrics = distillation_trainer.train_epoch(\n",
    "        train_dataset=train_dataset,\n",
    "        eval_prompts=epoch_eval,\n",
    "        epoch_num=epoch,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[Epoch {epoch}] Loss: {metrics.train_loss:.4f} | \"\n",
    "          f\"Avg Score: {metrics.avg_student_score:.2f}/10 | \"\n",
    "          f\"Corrections: {metrics.num_corrections} | \"\n",
    "          f\"Rate: {metrics.correction_rate:.1%}\")\n",
    "\n",
    "    converged, reason = distillation_trainer.check_convergence()\n",
    "    if converged:\n",
    "        print(f\"\\n[CONVERGED] {reason}\")\n",
    "        break\n",
    "\n",
    "summary = distillation_trainer.get_training_summary()\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "for k, v in summary.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation & Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the fine-tuned model on sample prompts\n",
    "model.eval()\n",
    "\n",
    "test_prompts = [\n",
    "    \"Generate C code for a TSN Time-Aware Shaper (802.1Qbv) gate control list entry struct.\",\n",
    "    \"Generate C code for AVB Stream Reservation Protocol talker advertisement.\",\n",
    "    \"Generate C code for IEEE 802.1AS PTP timestamp comparison function.\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"POST-TRAINING EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    if hasattr(tokenizer, 'apply_chat_template') and tokenizer.chat_template:\n",
    "        formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        formatted = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
    "\n",
    "    inputs = tokenizer(formatted, return_tensors='pt', truncation=True, max_length=4096)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs, max_new_tokens=512, do_sample=True,\n",
    "            temperature=0.7, top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    generated = tokenizer.decode(\n",
    "        output_ids[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    quality = evaluator.evaluate(generated, prompt)\n",
    "\n",
    "    print(f\"\\n--- Prompt {i+1} ---\")\n",
    "    print(f\"Prompt: {prompt[:80]}...\")\n",
    "    print(f\"Score: {quality}\")\n",
    "    print(f\"Output preview: {generated[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save & Export (S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model locally and upload to S3\n",
    "model_output_dir = os.path.join(PROJECT_ROOT, 'models', 'notebook_output')\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(model_output_dir)\n",
    "tokenizer.save_pretrained(model_output_dir)\n",
    "print(f\"Model saved locally: {model_output_dir}\")\n",
    "\n",
    "# Upload to S3\n",
    "s3_client = boto3.client('s3', region_name=os.environ.get('AWS_REGION', 'us-east-1'))\n",
    "s3_model_prefix = f\"{S3_OUTPUT_PREFIX}/models/notebook_output\"\n",
    "\n",
    "print(f\"\\nUploading model to s3://{OUTPUT_BUCKET}/{s3_model_prefix}/\")\n",
    "for f in Path(model_output_dir).glob('*'):\n",
    "    if f.is_file():\n",
    "        s3_key = f\"{s3_model_prefix}/{f.name}\"\n",
    "        print(f\"  {f.name} ({f.stat().st_size / 1e6:.1f} MB)\")\n",
    "        s3_client.upload_file(str(f), OUTPUT_BUCKET, s3_key)\n",
    "\n",
    "print(f\"Model uploaded to s3://{OUTPUT_BUCKET}/{s3_model_prefix}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training summary locally and upload to S3\n",
    "summary_file = os.path.join(output_dir, 'training_summary.json')\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "# Upload to S3\n",
    "s3_key = f\"{S3_OUTPUT_PREFIX}/summaries/training_summary.json\"\n",
    "s3_client.upload_file(summary_file, OUTPUT_BUCKET, s3_key)\n",
    "\n",
    "print(f\"Summary saved to: {summary_file}\")\n",
    "print(f\"Summary uploaded to: s3://{OUTPUT_BUCKET}/{s3_key}\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-257824287.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Upload model to S3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ms3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregion_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AWS_REGION'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'us-east-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aws'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bucket_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0ms3_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aws'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_prefix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/notebook-finetuned'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# Upload model to data S3 bucket (for downstream consumption)\n",
    "bucket = config['aws']['s3']['bucket_name']\n",
    "s3_prefix = config['aws']['s3'].get('model_prefix', 'models') + '/notebook-finetuned'\n",
    "\n",
    "print(f\"Uploading model to s3://{bucket}/{s3_prefix}/\")\n",
    "for file_path in Path(model_output_dir).glob('*'):\n",
    "    if file_path.is_file():\n",
    "        s3_key = f\"{s3_prefix}/{file_path.name}\"\n",
    "        print(f\"  {file_path.name} ({file_path.stat().st_size / 1e6:.1f} MB) -> s3://{bucket}/{s3_key}\")\n",
    "        s3_client.upload_file(str(file_path), bucket, s3_key)\n",
    "\n",
    "s3_client.upload_file(\n",
    "    os.path.join(output_dir, 'training_summary.json'),\n",
    "    bucket,\n",
    "    f\"{s3_prefix}/training_summary.json\",\n",
    ")\n",
    "\n",
    "print(f\"\\nModel uploaded to s3://{bucket}/{s3_prefix}/\")\n",
    "\n",
    "# Optional: Push to HuggingFace Hub\n",
    "PUSH_TO_HUB = False\n",
    "HUB_REPO_NAME = \"your-username/granite-8b-avb-tsn-finetuned\"\n",
    "\n",
    "if PUSH_TO_HUB:\n",
    "    api = HfApi()\n",
    "    api.create_repo(HUB_REPO_NAME, exist_ok=True, private=True)\n",
    "    model.push_to_hub(HUB_REPO_NAME, token=os.environ.get('HF_TOKEN'))\n",
    "    tokenizer.push_to_hub(HUB_REPO_NAME, token=os.environ.get('HF_TOKEN'))\n",
    "    print(f\"Model pushed to https://huggingface.co/{HUB_REPO_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}